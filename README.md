# Transformer

## Tokenization

考虑到计算机没有办法直接识别人类语言，我们将每一个词映射为一个token使得计算机可以直接识别。
为实现这个目的我们使用BPE算法将每个词划分为若干个前缀和后缀，以此拼起来一个词，节省vocabulary占用的空间。
这一部分写于BPE.h

## Word2Vec

现在我们把语言tokenize以后，我们希望得到词汇的词向量，以完成后续的embedding操作。
我们考虑如何处理词向量。首先一个比较简略的办法是one-hot表示法，有且仅有一个元素是1，其他全部置0，然后每一个单词的词向量都不相同。但是这样空间利用率不够高，并且也没有足够的区分度，所以我们考虑去使用一种distributed的表示法，让每一个维度都表示一个该词的属性，所有维度拼起来，就是这个词的全部意义。

所以我们现在补充一下前置知识，对于机器学习，我们评估它的训练质量是使用极大似然估计和损失函数来进行的。

e.g. 如果一个袋子里有若干个红球，若干个蓝球，取10次球，取到的蓝球个数为$x$，我们得到一个结果$E(x) = 7$，那么我们该怎么反推单次取蓝球的概率？
一个比较直观的想法是我们去让$P(x = 7)最大$。

也就是说，我们令$P(x = 7) = \binom{10}{7} p^7(1-p)^3$最大，其中$p$是单次取蓝球的概率。

由于是指数级的，我们取对数$\log P(x = 7) = \binom{10}{7}(7\log p + 3 \log (1-p))$

令其导数为0，即$7 \frac{1}{p} - 3 \frac{1}{1-p} = 0$，得到p = 0.7

推广一下，如果说现在有若干种颜色的球，也就是有若干种不同的可能性，每种可能称为$D_i$，且$D_i$满足伯努利分布。

那么我们考虑$P(X) = \binom{\sum_{i=1}^{n}|D_i|}{D_1,D_2,D_3,...,D_n}\prod_{i = 1}^{n}P(D_i)$

由于组合数是一个固定的常数，求导后置0的过程中与它的值无关，而且有的时候事情的发生是有顺序的，不需要乘上组合数的系数，我们大可以忽略掉。

因此我们可以定义$\log P(X) = \sum_{i = 1}^n \log (D_i) = \sum_{i = 1}^n (\log pD_i + \log(1-p) (1-D_i))$

由于我们要令这个值最大，所以也就是使得它的负值最小，于是$-\log p = -\sum_{i = 1}^n (\log pD_i + \log(1-p) (1-D_i))$定义为它的交叉熵损失函数，我们训练的时候要最小化这个函数的值。

不过在上面的例子中我们可以直接得出来p在最大值点的取值，依旧是求导。

$\sum_{i = 1}^n (D_i \frac{1}{p} - (1 - D_i) \frac{1}{1 - p})$

化简得：$p = \frac{1}{n}\sum_{i = 1}^nD_i$
